# 第一回お勉強会 2/11 @八木ちゃん家 やすお記
## 高平報告分
### 機械学習
- まずData自体の分析が必要(前処理)
- モデルの選択
- パラメータチューニング
### Titanicについて
- `train.csv`:正解有りデータからSurvivedを推測する1.0を目指す
- `test.csv`:正解なし
- つまりながれとしては 
  1. `train.csv`から教師有り学習をする
  2. trainモデルを訓練させる
  3. 実際に鍛えたtest.csvを食わせて，結果をあぶり出す
  4. 提出すると，kaggle側にある正解と比較．Max 1.0(100%)

```
y = f(x) 
x:説明変数, y:目的変数 f():trainモデル
```
### Data Setの説明
- Pclass: 階級
- Sex
- Nameの中にMiss，Mr，Master，Mrs，Doctor．．．その他
- ダミーを作る000000．Mrなら010000
- その他
### データの欠損について
- 規則性のある欠損
  - 他のカラムに依存して欠損したもの
  - 自身の外部のルールに基づいて落ちる
- ランダムに落ちたもの

- 年齢とりあえず今回は平均値を打ち込んだ

### モデル
- XGBClassifier
  - 結構強いらしい
  - 決定木の組み合わせみたいなやつ
  - 多数決．アンサンブリング
- グリッドサーチ
- CV:クロスバリデーション
  - 例:モデル全体を5個に分割して，4/5を学習1/5で検証．
  - そのパターンを5回．その後平均
- アンサンブリングはいろんな意見があったほうがよい結果がでる
- hyperopt
  - グリッドサーチをするためのざっくりしたパラメータの範囲を絞る
- 学習木ごとにpro/conがありそう．

### 未分類
- RBF kernel
- 学習データと検証用データ両方に使うべきではない
  - 過学習
- ドメイン知識

### 今の性能
- submitした後は0.764くらい
### 今後の課題
- 欠損予測
  - nameのMrとか?
  - embarkedをダミー化してる

## やぎちゃんの報告
- スライド参照
### コメント．その他
- Jupiterのノートわかりやすい
- 分類と回帰
- Fareの値なんかへんじゃない?

